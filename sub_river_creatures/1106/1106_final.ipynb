{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eee1179",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Tosha.E.T\\\\Desktop\\\\工作\\\\eDNA\\\\sub_river_creatures\\\\1106\\\\river_creature_22.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0b85e74b9103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# READ RAWDATA   df_all = pd.read_csv(r'YOUR DATA PATH')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdf_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'C:\\Users\\Tosha.E.T\\Desktop\\工作\\eDNA\\sub_river_creatures\\1106\\river_creature_22.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mdf_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1448\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m             self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    861\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Tosha.E.T\\\\Desktop\\\\工作\\\\eDNA\\\\sub_river_creatures\\\\1106\\\\river_creature_22.csv'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# SUBRIVER LIST (13)\n",
    "CuKeng\n",
    "NanShi\n",
    "SongLuo\n",
    "FanFan\n",
    "SanGuang\n",
    "BaoYan\n",
    "FuBuEr\n",
    "MeiYou\n",
    "QingShui\n",
    "TianGou\n",
    "TuChang\n",
    "MeiLuo\n",
    "TaiGang\n",
    "'''\n",
    "import pandas as pd\n",
    "\n",
    "# READ RAWDATA   df_all = pd.read_csv(r'YOUR DATA PATH')\n",
    "\n",
    "df_all = pd.read_csv(r'C:\\Users\\Tosha.E.T\\Desktop\\工作\\eDNA\\sub_river_creatures\\1106\\river_creature_22.csv')\n",
    "df_all = pd.DataFrame(df_all)\n",
    "\n",
    "\n",
    "# SEPERATE RIVERS   df_XXX = df_all[df_all['river'] == \"XXX\"].reset_index(drop=True)\n",
    "\n",
    "df_CuKeng = df_all[df_all['river'] == \"CuKeng\"].reset_index(drop=True)\n",
    "df_NanShi = df_all[df_all['river'] == \"NanShi\"].reset_index(drop=True)\n",
    "df_SongLuo = df_all[df_all['river'] == \"SongLuo\"].reset_index(drop=True)\n",
    "df_FanFan = df_all[df_all['river'] == \"FanFan\"].reset_index(drop=True)\n",
    "df_SanGuang = df_all[df_all['river'] == \"SanGuang\"].reset_index(drop=True)\n",
    "df_BaoYan = df_all[df_all['river'] == \"BaoYan\"].reset_index(drop=True)\n",
    "df_FuBuEr = df_all[df_all['river'] == \"FuBuEr\"].reset_index(drop=True)\n",
    "df_MeiYou = df_all[df_all['river'] == \"MeiYou\"].reset_index(drop=True)\n",
    "df_QingShui = df_all[df_all['river'] == \"QingShui\"].reset_index(drop=True)\n",
    "df_TianGou = df_all[df_all['river'] == \"TianGou\"].reset_index(drop=True)\n",
    "df_TuChang = df_all[df_all['river'] == \"TuChang\"].reset_index(drop=True)\n",
    "df_MeiLuo = df_all[df_all['river'] == \"MeiLuo\"].reset_index(drop=True)\n",
    "df_TaiGang = df_all[df_all['river'] == \"TaiGang\"].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# FIND SPECIES   species_XXX = df_XXX['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "\n",
    "species_all = df_all['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "\n",
    "species_CuKeng = df_CuKeng['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_NanShi = df_NanShi['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_SongLuo = df_SongLuo['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_FanFan = df_FanFan['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_SanGuang = df_SanGuang['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_BaoYan = df_BaoYan['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_FuBuEr = df_FuBuEr['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_MeiYou = df_MeiYou['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_QingShui = df_QingShui['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_TianGou = df_TianGou['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_TuChang = df_TuChang['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_MeiLuo = df_MeiLuo['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "species_TaiGang = df_TaiGang['scientific'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "\n",
    "\n",
    "# MAKE SPECIES 01 LIST\n",
    "'''\n",
    "list_XXX=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_XXX):\n",
    "        #print(species_all[i])\n",
    "        list_XXX.append(1)\n",
    "    else:\n",
    "        list_XXX.append(0)\n",
    "'''\n",
    "\n",
    "list_CuKeng=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_CuKeng):\n",
    "        #print(species_all[i])\n",
    "        list_CuKeng.append(1)\n",
    "    else:\n",
    "        list_CuKeng.append(0)\n",
    "\n",
    "list_NanShi=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_NanShi):\n",
    "        #print(species_all[i])\n",
    "        list_NanShi.append(1)\n",
    "    else:\n",
    "        list_NanShi.append(0)\n",
    "\n",
    "list_SongLuo=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_SongLuo):\n",
    "        #print(species_all[i])\n",
    "        list_SongLuo.append(1)\n",
    "    else:\n",
    "        list_SongLuo.append(0)\n",
    "        \n",
    "list_FanFan=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_FanFan):\n",
    "        #print(species_all[i])\n",
    "        list_FanFan.append(1)\n",
    "    else:\n",
    "        list_FanFan.append(0)\n",
    "\n",
    "\n",
    "list_SanGuang=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_SanGuang):\n",
    "        #print(species_all[i])\n",
    "        list_SanGuang.append(1)\n",
    "    else:\n",
    "        list_SanGuang.append(0)\n",
    "        \n",
    "list_BaoYan=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_BaoYan):\n",
    "        #print(species_all[i])\n",
    "        list_BaoYan.append(1)\n",
    "    else:\n",
    "        list_BaoYan.append(0)\n",
    "        \n",
    "list_FuBuEr=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_FuBuEr):\n",
    "        #print(species_all[i])\n",
    "        list_FuBuEr.append(1)\n",
    "    else:\n",
    "        list_FuBuEr.append(0)\n",
    "        \n",
    "list_MeiYou=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_MeiYou):\n",
    "        #print(species_all[i])\n",
    "        list_MeiYou.append(1)\n",
    "    else:\n",
    "        list_MeiYou.append(0)\n",
    "\n",
    "list_QingShui=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_QingShui):\n",
    "        #print(species_all[i])\n",
    "        list_QingShui.append(1)\n",
    "    else:\n",
    "        list_QingShui.append(0)\n",
    "        \n",
    "list_TianGou=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_TianGou):\n",
    "        #print(species_all[i])\n",
    "        list_TianGou.append(1)\n",
    "    else:\n",
    "        list_TianGou.append(0)\n",
    "        \n",
    "list_TuChang=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_TuChang):\n",
    "        #print(species_all[i])\n",
    "        list_TuChang.append(1)\n",
    "    else:\n",
    "        list_TuChang.append(0)\n",
    "        \n",
    "list_MeiLuo=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_MeiLuo):\n",
    "        #print(species_all[i])\n",
    "        list_MeiLuo.append(1)\n",
    "    else:\n",
    "        list_MeiLuo.append(0)\n",
    "        \n",
    "list_TaiGang=[]\n",
    "for i in range(len(species_all)):\n",
    "    if(species_all[i] in species_TaiGang):\n",
    "        #print(species_all[i])\n",
    "        list_TaiGang.append(1)\n",
    "    else:\n",
    "        list_TaiGang.append(0)\n",
    "        \n",
    "\n",
    "        \n",
    "# SAVE TO CSV\n",
    "\n",
    "dict = {'all': species_all, 'BaoYan': list_BaoYan, 'CuKeng': list_CuKeng, 'FanFan': list_FanFan, \n",
    "        'FuBuEr': list_FuBuEr, 'MeiLuo': list_MeiLuo, 'MeiYou': list_MeiYou,\n",
    "       'NanShi': list_NanShi, 'QingShui': list_QingShui, 'SongLuo': list_SongLuo,\n",
    "       'SanGuang': list_SanGuang, 'TianGou': list_TianGou, 'TuChang': list_TuChang,\n",
    "       'TaiGang': list_TaiGang}\n",
    "     \n",
    "df = pd.DataFrame(dict)\n",
    "df = df.T\n",
    "\n",
    "df.to_csv('result.csv', header=False)\n",
    "print(\"result.csv SAVE!\")\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "# STATISTICAL DATA \n",
    "'''\n",
    "list_class_XXX = [0]*len(class_all)\n",
    "for i in range(len(df_XXX)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_XXX['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_XXX[j] += 1;\n",
    "'''\n",
    "class_all = df_all['taxon_clas'].drop_duplicates().reset_index(drop=True).tolist()\n",
    "\n",
    "list_class_CuKeng = [0]*len(class_all)\n",
    "for i in range(len(df_CuKeng)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_CuKeng['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_CuKeng[j] += 1;\n",
    "            \n",
    "list_class_NanShi = [0]*len(class_all)\n",
    "for i in range(len(df_NanShi)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_NanShi['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_NanShi[j] += 1;\n",
    "            \n",
    "list_class_SongLuo = [0]*len(class_all)\n",
    "for i in range(len(df_SongLuo)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_SongLuo['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_SongLuo[j] += 1;\n",
    "            \n",
    "list_class_FanFan = [0]*len(class_all)\n",
    "for i in range(len(df_FanFan)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_FanFan['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_FanFan[j] += 1;            \n",
    "\n",
    "list_class_SanGuang = [0]*len(class_all)\n",
    "for i in range(len(df_SanGuang)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_SanGuang['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_SanGuang[j] += 1;\n",
    "            \n",
    "list_class_BaoYan = [0]*len(class_all)\n",
    "for i in range(len(df_BaoYan)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_BaoYan['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_BaoYan[j] += 1;\n",
    "            \n",
    "list_class_FuBuEr = [0]*len(class_all)\n",
    "for i in range(len(df_FuBuEr)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_FuBuEr['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_FuBuEr[j] += 1;           \n",
    "            \n",
    "list_class_MeiYou = [0]*len(class_all)\n",
    "for i in range(len(df_MeiYou)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_MeiYou['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_MeiYou[j] += 1;\n",
    "            \n",
    "list_class_QingShui = [0]*len(class_all)\n",
    "for i in range(len(df_QingShui)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_QingShui['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_QingShui[j] += 1;\n",
    "            \n",
    "list_class_TianGou = [0]*len(class_all)\n",
    "for i in range(len(df_TianGou)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_TianGou['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_TianGou[j] += 1;             \n",
    "            \n",
    "list_class_TuChang = [0]*len(class_all)\n",
    "for i in range(len(df_TuChang)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_TuChang['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_TuChang[j] += 1;\n",
    "            \n",
    "list_class_MeiLuo = [0]*len(class_all)\n",
    "for i in range(len(df_MeiLuo)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_MeiLuo['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_MeiLuo[j] += 1;   \n",
    "\n",
    "\n",
    "list_class_TaiGang = [0]*len(class_all)\n",
    "for i in range(len(df_TaiGang)):\n",
    "    for j in range(len(class_all)):\n",
    "        if(df_TaiGang['taxon_clas'][i] in class_all[j]):\n",
    "            list_class_TaiGang[j] += 1;\n",
    "            \n",
    "\n",
    "dict = {'all': class_all, 'BaoYan': list_class_BaoYan, 'CuKeng': list_class_CuKeng, 'FanFan': list_class_FanFan, \n",
    "        'FuBuEr': list_class_FuBuEr, 'MeiLuo': list_class_MeiLuo, 'MeiYou': list_class_MeiYou,\n",
    "       'NanShi': list_class_NanShi, 'QingShui': list_class_QingShui, 'SongLuo': list_class_SongLuo,\n",
    "       'SanGuang': list_class_SanGuang, 'TianGou': list_class_TianGou, 'TuChang': list_class_TuChang,\n",
    "       'TaiGang': list_class_TaiGang}\n",
    "     \n",
    "df = pd.DataFrame(dict)\n",
    "df = df.T\n",
    "\n",
    "df.to_csv('StatisticalData.csv', header=False)\n",
    "print(\"StatisticalData.csv SAVE!\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5667b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
